# -*- coding: utf-8 -*-
"""HumantoAnime Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fD8qwS6lBbZyk8DZCbhdfxiotE2ZuIED
"""

import tensorflow as tf
import numpy as np
from keras.layers import Dense,Conv2D,BatchNormalization,Conv2DTranspose,Add,MaxPooling2D,GlobalAveragePooling2D,Dropout
from tensorflow.keras.optimizers import Adam
from keras.models import Model,Input
import matplotlib.pyplot as plt
from google.colab import drive
from keras.preprocessing.image import load_img,img_to_array
import os
import matplotlib.pyplot as pyplot
from keras.initializers import RandomNormal

drive.mount('/content/drive')

!unzip /content/drive/MyDrive/CycleGan/anime.zip -d /content/sample_data

!unzip /content/drive/MyDrive/CycleGan/human.zip -d /content/sample_data

path = '/content/sample_data'

def RealData(batch_size,cat='anime'):
  random = np.random.randint(0,9780,batch_size)
  generatedData = []
  for i in np.array(os.listdir(os.path.join(path,cat)))[random]:
    filename = os.path.join(os.path.join(path,cat),i)
    img = load_img(filename,target_size=(64,64))
    imgPixel = img_to_array(img)/255
    generatedData.append(imgPixel)
  return np.asarray(generatedData)

def FakeData(batch_size,model,data):
  X = model.predict(data)
  return X

def discriminator(image_shape=(256,256,3)):
  init = RandomNormal(stddev=0.02)
  inputs = Input(shape=image_shape)
  x = Conv2D(64,kernel_size=(3,3),strides=(2,2),padding='same',activation='relu', kernel_initializer=init)(inputs)
  x = BatchNormalization()(x)
  x = MaxPooling2D()(x)
  x = Conv2D(128,kernel_size=(3,3),strides=(2,2),padding='same',activation='relu', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = MaxPooling2D()(x)
  x = Conv2D(256,kernel_size=(3,3),strides=(2,2),padding='same',activation='relu', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = GlobalAveragePooling2D()(x)
  x = Dense(64,activation='relu', kernel_initializer=init)(x)
  x = Dropout(0.4)(x)
  outputs = Dense(1,activation='sigmoid')(x)

  model = Model(inputs,outputs)
  model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')
  return model

def resnet(n_filters,input_layer):
  # weight initialization
	init = RandomNormal(stddev=0.02)
	# first layer convolutional layer
	g = Conv2D(n_filters, (3,3), padding='same',activation='relu', kernel_initializer=init)(input_layer)
	g = BatchNormalization()(g)
	# second convolutional layer
	g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)
	g = BatchNormalization()(g)
	# concatenate merge channel-wise with input layer
	g = Add()([g, input_layer])
	return g

def generator(image_shape):
  init = RandomNormal(stddev=0.02)
  inputs = Input(shape=image_shape)
  x = Conv2D(64,kernel_size=(3,3),padding='same',activation='relu', kernel_initializer=init)(inputs)
  x = BatchNormalization()(x)
  x = MaxPooling2D()(x)
  x = Conv2D(128,kernel_size=(3,3),padding='same',activation='relu', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = MaxPooling2D()(x)
  x = Conv2D(256,kernel_size=(3,3),padding='same',activation='relu', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = MaxPooling2D()(x)
  for _ in range(20):
	  x = resnet(256, x)
  x = Conv2DTranspose(64,kernel_size=(3,3),strides=(2,2),padding='same', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = Conv2DTranspose(128,kernel_size=(3,3),strides=(2,2),padding='same', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = Conv2DTranspose(128,kernel_size=(3,3),strides=(2,2),padding='same', kernel_initializer=init)(x)
  x = BatchNormalization()(x)
  x = Conv2DTranspose(3,kernel_size=(3,3),strides=(1,1),padding='same',activation='sigmoid', kernel_initializer=init)(x)
  model = Model(inputs,x)
  return model

def compositeNetwork(gen_model1,d_model,gen_model2,image_shape):
  gen_model1.trainable = True #pelajari generator saja
  gen_model1.trainable = False
  d_model.trainable = False

  
  inputDomain1 = Input(shape=image_shape)
  inputDomain2 = Input(shape=image_shape)

  #adversial
  aModel = gen_model1(inputDomain1)
  aoutput = d_model(aModel)

  # identity
  ioutput = gen_model1(inputDomain2)

  #forward
  foutput = gen_model2(aModel)

  #backward
  bModel = gen_model2(inputDomain2)
  boutput = gen_model1(bModel)

  model = Model([inputDomain1,inputDomain2],[aoutput,ioutput,foutput,boutput])
  model.compile(optimizer=Adam(learning_rate=0.02),loss=['binary_crossentropy','mse','mse','mse'],loss_weights=[1,5,10,10])
  return model

def train(discriminatorA,discriminatorB,generatorB2A,generatorA2B,CModelB2A,CModelA2B):
  step = 100
  batch = 10
  for j in range(step):
    #a = human , b = anime
    XRealA,XRealB,YRealA,YRealB = (RealData(batch,cat='human'),
                                    RealData(batch,cat='anime'),
                                    np.ones(batch).reshape(-1,1),
                                    np.ones(batch).reshape(-1,1)
                                    )
    
    XFakeA,XFakeB,YFakeA,YFakeB = (FakeData(batch,generatorB2A,XRealB),
                                    FakeData(batch,generatorA2B,XRealA),
                                    np.zeros(batch).reshape(-1,1),
                                    np.zeros(batch).reshape(-1,1)
                                    )
    cAModel,_,_,_,_ = CModelB2A.train_on_batch([XRealB,XRealA],[YRealA,XRealA,XRealB,XRealA])
    cBModel,_,_,_,_ = CModelA2B.train_on_batch([XRealA,XRealB],[YRealB,XRealB,XRealA,XRealB])
    XA,YA = np.vstack((XRealA,XFakeA)), np.vstack((YRealA,YFakeA))
    XB,YB = np.vstack((XRealB,XFakeB)), np.vstack((YRealB,YFakeB))
    dAloss = discriminatorA.train_on_batch(XA,YA)
    dBloss = discriminatorB.train_on_batch(XB,YB)
    print('>%d, dA[%.3f] dB[%.3f] g[%.3f,%.3f]' % (j+1, dAloss,dBloss,cAModel,cBModel))
    if j==99:
      filename1 = 'g_model_AtoB.h5'
      generatorA2B.save(filename1)
      filename2 = 'g_model_BtoA.h5'
      generatorB2A.save(filename2)

discriminatorA,discriminatorB = discriminator(image_shape=(64,64,3)),discriminator(image_shape=(64,64,3))
generatorA, generatorB = generator(image_shape=(64,64,3)),generator(image_shape=(64,64,3))
cModelA2B,cModelB2A = compositeNetwork(generatorB,discriminatorB,generatorA,image_shape=(64,64,3)),compositeNetwork(generatorA,discriminatorA,generatorB,image_shape=(64,64,3))

train(discriminatorA, discriminatorB, generatorA, generatorB, cModelB2A, cModelA2B)

import cv2

from google.colab.patches import cv2_imshow

a = RealData(1,cat='anime')

cv2_imshow(cv2.cvtColor(generatorA.predict(a)[0]*255,cv2.COLOR_RGB2BGR))

cv2_imshow(cv2.cvtColor(a[0]*255,cv2.COLOR_RGB2BGR))

a = RealData(1,cat='anime')

cv2_imshow(cv2.cvtColor(generatorA.predict(a)[0]*255,cv2.COLOR_RGB2BGR))

cv2_imshow(cv2.cvtColor(a[0]*255,cv2.COLOR_RGB2BGR))

a = RealData(1,cat='anime')

cv2_imshow(cv2.cvtColor(generatorA.predict(a)[0]*255,cv2.COLOR_RGB2BGR))

cv2_imshow(cv2.cvtColor(a[0]*255,cv2.COLOR_RGB2BGR))